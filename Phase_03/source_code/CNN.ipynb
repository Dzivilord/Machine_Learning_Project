{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c654aec",
   "metadata": {},
   "source": [
    "# Phase 3: ADVANCED ARCHITECTURE NEURAL NETWORK- CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a0a64",
   "metadata": {},
   "source": [
    "Tên và MSSV của từng thành viên:\n",
    "- Đinh Viết Lợi - 22120188.\n",
    "- Nguyễn Trần Lợi - 22120190.\n",
    "- Nguyễn Nhật Long - 22120194.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e2c08",
   "metadata": {},
   "source": [
    "## Nắm yêu cầu của Phase 3:\n",
    "Đây là bài toán phân loại nhận biết đoạn âm thanh có chứa tiếng gà tây hay không bằng cách sử dụng các kiến trúc neural network nâng cao, notebook này sẽ sử dụng CNN và tuân theo quy trình: Phân tích Dữ liệu Khám phá (Exploratory Data Analysis), Phát triển Mô hình (Model Development) và Đánh giá Mô hình (Model Evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02cefb",
   "metadata": {},
   "source": [
    "# Import thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "203c7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing  import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ef5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tracemalloc\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db93ed",
   "metadata": {},
   "source": [
    "# Đọc dữ liệu đầu vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a261931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../../dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50274dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + '/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(data_path + '/test.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6b1a8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81a7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.DataFrame(train_data)\n",
    "test_data= pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b0c5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1195 entries, 0 to 1194\n",
      "Data columns (total 5 columns):\n",
      " #   Column                           Non-Null Count  Dtype \n",
      "---  ------                           --------------  ----- \n",
      " 0   audio_embedding                  1195 non-null   object\n",
      " 1   is_turkey                        1195 non-null   int64 \n",
      " 2   vid_id                           1195 non-null   object\n",
      " 3   end_time_seconds_youtube_clip    1195 non-null   int64 \n",
      " 4   start_time_seconds_youtube_clip  1195 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 46.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d3bfe",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d6246e",
   "metadata": {},
   "source": [
    "padding dữ liệu, đệm thêm giá trị để các mẫu đều có cùng kích thước, cụ thể là các giá trị của thuộc tính `audio_embedding` sẽ có kích thước là 10*128 (10 giây và mỗi giây được biểu thị bởi vector 128 chiều)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7a5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings=train_data['audio_embedding'].apply(np.array)\n",
    "max_len = train_embeddings.apply(len).max()\n",
    "embedding_dim = len(train_embeddings.iloc[0][0])\n",
    "train_X = pad_sequences(train_embeddings, maxlen=max_len, dtype='float32', padding='post', truncating='post')\n",
    "train_Y = train_data['is_turkey'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c2f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = test_data['audio_embedding'].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "test_embeddings = test_data.loc[valid_idx, 'audio_embedding'].apply(np.array)\n",
    "test_X = pad_sequences(test_embeddings, maxlen=max_len, dtype='float32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf160cd",
   "metadata": {},
   "source": [
    "Chuẩn hoá dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4398641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, D = train_X.shape\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X.reshape(-1, D)).reshape(B, T, D)\n",
    "test_X = scaler.transform(test_X.reshape(-1, D)).reshape(test_X.shape[0], T, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6756c13",
   "metadata": {},
   "source": [
    "dữ liệu sau chuẩn hoá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe39e028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7394052 , -0.21803653,  0.9535418 , ...,  0.17066261,\n",
       "        -1.0943216 ,  0.48807013],\n",
       "       [ 0.71722543, -0.12128289,  0.5300489 , ...,  0.13660932,\n",
       "         1.047936  ,  0.48807013],\n",
       "       [ 0.6728659 , -0.23738725,  0.55496025, ...,  1.0333463 ,\n",
       "        -0.7566832 ,  0.48807013],\n",
       "       ...,\n",
       "       [ 0.561967  , -0.3534916 ,  0.57987165, ...,  0.31822693,\n",
       "         0.4308726 ,  0.48807013],\n",
       "       [ 0.4510681 , -0.46959594,  0.00691055, ...,  0.0230983 ,\n",
       "         1.362289  ,  0.48807013],\n",
       "       [ 0.5841468 , -0.23738725,  0.25602406, ..., -0.14716822,\n",
       "         0.06994878,  0.48807013]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7ace5",
   "metadata": {},
   "source": [
    "# Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef380f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 1: Split gốc\n",
    "train_X_, val_X, train_Y_, val_Y = train_test_split(train_X, train_Y, test_size=0.3, random_state=45)\n",
    "\n",
    "# Bước 2: Augment **chỉ tập train**\n",
    "noise = np.random.normal(0, 0.01, size=train_X_.shape)\n",
    "train_X_noisy = train_X_ + noise\n",
    "\n",
    "# Bước 3: Nối lại tập train mở rộng\n",
    "X_tr_aug = np.concatenate([train_X_, train_X_noisy], axis=0)\n",
    "y_tr_aug = np.concatenate([train_Y_, train_Y_], axis=0)\n",
    "\n",
    "# Bước 4: reshape & convert sang Tensor\n",
    "X_tr_cnn = torch.tensor(X_tr_aug.reshape(-1, T, D), dtype=torch.float32)\n",
    "y_tr = torch.tensor(y_tr_aug, dtype=torch.float32)\n",
    "\n",
    "X_val_cnn = torch.tensor(val_X.reshape(-1, T, D), dtype=torch.float32)\n",
    "y_val = torch.tensor(val_Y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2760dc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b773fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1672, 10, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_cnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cafbb0",
   "metadata": {},
   "source": [
    "## Xây dựng mô hình mạng Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb72ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.pool(x).squeeze(-1)\n",
    "        w = self.fc(w).unsqueeze(-1)\n",
    "        return x * w\n",
    "\n",
    "class AudioCNNWithSE(nn.Module):\n",
    "    def __init__(self, in_channels=128, out_channels=64, fc_out=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, 3, padding=1)\n",
    "        self.se1 = SEBlock(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels * 2, 3, padding=1)\n",
    "        self.se2 = SEBlock(out_channels * 2)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(out_channels * 2, fc_out)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.drop(self.se1(F.relu(self.conv1(x))))\n",
    "        x = self.drop(self.se2(F.relu(self.conv2(x))))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba21e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioCNNWithSE()  # or num_classes=3 if 3 classes\n",
    "criterion = nn.BCEWithLogitsLoss()  # dùng nếu output là sigmoid (nhị phân)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5,weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626d259",
   "metadata": {},
   "source": [
    "## Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe66c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.6911, Val Loss: 0.6899\n",
      "Epoch 1, Train Loss: 0.6891, Val Loss: 0.6879\n",
      "Epoch 2, Train Loss: 0.6872, Val Loss: 0.6858\n",
      "Epoch 3, Train Loss: 0.6855, Val Loss: 0.6836\n",
      "Epoch 4, Train Loss: 0.6834, Val Loss: 0.6812\n",
      "Epoch 5, Train Loss: 0.6809, Val Loss: 0.6785\n",
      "Epoch 6, Train Loss: 0.6783, Val Loss: 0.6756\n",
      "Epoch 7, Train Loss: 0.6751, Val Loss: 0.6723\n",
      "Epoch 8, Train Loss: 0.6718, Val Loss: 0.6687\n",
      "Epoch 9, Train Loss: 0.6684, Val Loss: 0.6647\n",
      "Epoch 10, Train Loss: 0.6646, Val Loss: 0.6602\n",
      "Epoch 11, Train Loss: 0.6597, Val Loss: 0.6551\n",
      "Epoch 12, Train Loss: 0.6545, Val Loss: 0.6496\n",
      "Epoch 13, Train Loss: 0.6489, Val Loss: 0.6433\n",
      "Epoch 14, Train Loss: 0.6423, Val Loss: 0.6365\n",
      "Epoch 15, Train Loss: 0.6358, Val Loss: 0.6290\n",
      "Epoch 16, Train Loss: 0.6284, Val Loss: 0.6209\n",
      "Epoch 17, Train Loss: 0.6204, Val Loss: 0.6122\n",
      "Epoch 18, Train Loss: 0.6113, Val Loss: 0.6027\n",
      "Epoch 19, Train Loss: 0.6021, Val Loss: 0.5927\n",
      "Epoch 20, Train Loss: 0.5920, Val Loss: 0.5819\n",
      "Epoch 21, Train Loss: 0.5812, Val Loss: 0.5709\n",
      "Epoch 22, Train Loss: 0.5702, Val Loss: 0.5590\n",
      "Epoch 23, Train Loss: 0.5583, Val Loss: 0.5467\n",
      "Epoch 24, Train Loss: 0.5461, Val Loss: 0.5339\n",
      "Epoch 25, Train Loss: 0.5334, Val Loss: 0.5208\n",
      "Epoch 26, Train Loss: 0.5204, Val Loss: 0.5072\n",
      "Epoch 27, Train Loss: 0.5069, Val Loss: 0.4933\n",
      "Epoch 28, Train Loss: 0.4936, Val Loss: 0.4795\n",
      "Epoch 29, Train Loss: 0.4800, Val Loss: 0.4655\n",
      "Epoch 30, Train Loss: 0.4673, Val Loss: 0.4515\n",
      "Epoch 31, Train Loss: 0.4530, Val Loss: 0.4375\n",
      "Epoch 32, Train Loss: 0.4400, Val Loss: 0.4238\n",
      "Epoch 33, Train Loss: 0.4256, Val Loss: 0.4101\n",
      "Epoch 34, Train Loss: 0.4136, Val Loss: 0.3968\n",
      "Epoch 35, Train Loss: 0.4001, Val Loss: 0.3838\n",
      "Epoch 36, Train Loss: 0.3879, Val Loss: 0.3711\n",
      "Epoch 37, Train Loss: 0.3754, Val Loss: 0.3588\n",
      "Epoch 38, Train Loss: 0.3648, Val Loss: 0.3469\n",
      "Epoch 39, Train Loss: 0.3532, Val Loss: 0.3355\n",
      "Epoch 40, Train Loss: 0.3423, Val Loss: 0.3242\n",
      "Epoch 41, Train Loss: 0.3321, Val Loss: 0.3135\n",
      "Epoch 42, Train Loss: 0.3214, Val Loss: 0.3034\n",
      "Epoch 43, Train Loss: 0.3119, Val Loss: 0.2936\n",
      "Epoch 44, Train Loss: 0.3024, Val Loss: 0.2843\n",
      "Epoch 45, Train Loss: 0.2952, Val Loss: 0.2754\n",
      "Epoch 46, Train Loss: 0.2866, Val Loss: 0.2670\n",
      "Epoch 47, Train Loss: 0.2786, Val Loss: 0.2588\n",
      "Epoch 48, Train Loss: 0.2708, Val Loss: 0.2512\n",
      "Epoch 49, Train Loss: 0.2642, Val Loss: 0.2439\n",
      "Epoch 50, Train Loss: 0.2565, Val Loss: 0.2370\n",
      "Epoch 51, Train Loss: 0.2506, Val Loss: 0.2305\n",
      "Epoch 52, Train Loss: 0.2443, Val Loss: 0.2244\n",
      "Epoch 53, Train Loss: 0.2385, Val Loss: 0.2186\n",
      "Epoch 54, Train Loss: 0.2341, Val Loss: 0.2132\n",
      "Epoch 55, Train Loss: 0.2284, Val Loss: 0.2080\n",
      "Epoch 56, Train Loss: 0.2249, Val Loss: 0.2031\n",
      "Epoch 57, Train Loss: 0.2191, Val Loss: 0.1984\n",
      "Epoch 58, Train Loss: 0.2136, Val Loss: 0.1940\n",
      "Epoch 59, Train Loss: 0.2105, Val Loss: 0.1899\n",
      "Epoch 60, Train Loss: 0.2072, Val Loss: 0.1859\n",
      "Epoch 61, Train Loss: 0.2032, Val Loss: 0.1824\n",
      "Epoch 62, Train Loss: 0.2000, Val Loss: 0.1788\n",
      "Epoch 63, Train Loss: 0.1969, Val Loss: 0.1755\n",
      "Epoch 64, Train Loss: 0.1938, Val Loss: 0.1723\n",
      "Epoch 65, Train Loss: 0.1907, Val Loss: 0.1693\n",
      "Epoch 66, Train Loss: 0.1874, Val Loss: 0.1664\n",
      "Epoch 67, Train Loss: 0.1857, Val Loss: 0.1636\n",
      "Epoch 68, Train Loss: 0.1819, Val Loss: 0.1611\n",
      "Epoch 69, Train Loss: 0.1802, Val Loss: 0.1587\n",
      "Epoch 70, Train Loss: 0.1787, Val Loss: 0.1565\n",
      "Epoch 71, Train Loss: 0.1767, Val Loss: 0.1544\n",
      "Epoch 72, Train Loss: 0.1737, Val Loss: 0.1523\n",
      "Epoch 73, Train Loss: 0.1727, Val Loss: 0.1503\n",
      "Epoch 74, Train Loss: 0.1692, Val Loss: 0.1486\n",
      "Epoch 75, Train Loss: 0.1682, Val Loss: 0.1468\n",
      "Epoch 76, Train Loss: 0.1671, Val Loss: 0.1452\n",
      "Epoch 77, Train Loss: 0.1653, Val Loss: 0.1434\n",
      "Epoch 78, Train Loss: 0.1637, Val Loss: 0.1419\n",
      "Epoch 79, Train Loss: 0.1629, Val Loss: 0.1405\n",
      "Epoch 80, Train Loss: 0.1597, Val Loss: 0.1393\n",
      "Epoch 81, Train Loss: 0.1598, Val Loss: 0.1380\n",
      "Epoch 82, Train Loss: 0.1573, Val Loss: 0.1367\n",
      "Epoch 83, Train Loss: 0.1552, Val Loss: 0.1355\n",
      "Epoch 84, Train Loss: 0.1552, Val Loss: 0.1342\n",
      "Epoch 85, Train Loss: 0.1527, Val Loss: 0.1332\n",
      "Epoch 86, Train Loss: 0.1519, Val Loss: 0.1321\n",
      "Epoch 87, Train Loss: 0.1502, Val Loss: 0.1310\n",
      "Epoch 88, Train Loss: 0.1506, Val Loss: 0.1301\n",
      "Epoch 89, Train Loss: 0.1494, Val Loss: 0.1292\n",
      "Epoch 90, Train Loss: 0.1474, Val Loss: 0.1282\n",
      "Epoch 91, Train Loss: 0.1471, Val Loss: 0.1276\n",
      "Epoch 92, Train Loss: 0.1456, Val Loss: 0.1268\n",
      "Epoch 93, Train Loss: 0.1453, Val Loss: 0.1260\n",
      "Epoch 94, Train Loss: 0.1439, Val Loss: 0.1252\n",
      "Epoch 95, Train Loss: 0.1438, Val Loss: 0.1245\n",
      "Epoch 96, Train Loss: 0.1418, Val Loss: 0.1238\n",
      "Epoch 97, Train Loss: 0.1420, Val Loss: 0.1231\n",
      "Epoch 98, Train Loss: 0.1394, Val Loss: 0.1225\n",
      "Epoch 99, Train Loss: 0.1400, Val Loss: 0.1219\n",
      "Training time: 80.85 seconds\n",
      "Peak memory usage: 6.33 MB\n"
     ]
    }
   ],
   "source": [
    "tracemalloc.start()\n",
    "start_time = time.time()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "train_dataset = TensorDataset(X_tr_cnn, y_tr)\n",
    "val_dataset = TensorDataset(X_val_cnn, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb).squeeze()\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            val_outputs = model(xb).squeeze()\n",
    "            loss = criterion(val_outputs, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "end_time = time.time()\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Peak memory usage: {peak / 1024 / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1d3d4",
   "metadata": {},
   "source": [
    "Lượng tài nguyên sử dụng cho việc huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789d07b",
   "metadata": {},
   "source": [
    "## Dự đoán trên tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30cec138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9928\n",
      "Validation Precision: 0.9441\n",
      "Validation Accuracy: 0.9554\n",
      "Validation F1 Score: 0.9441\n",
      "Validation Recall: 0.9441\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_cnn).squeeze()\n",
    "    val_probs = torch.sigmoid(val_outputs).cpu().numpy()\n",
    "    val_preds = (val_probs > 0.5)\n",
    "    y_true = y_val.cpu().numpy()\n",
    "    acc = accuracy_score(y_true, val_preds)\n",
    "    f1 = f1_score(y_true, val_preds)\n",
    "    precision = precision_score(y_true, val_preds)\n",
    "    recall = recall_score(y_true, val_preds)\n",
    "    auc = roc_auc_score(y_true, val_probs)\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e8b07",
   "metadata": {},
   "source": [
    "## Lưu kết quả huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5857ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "    test_outputs = model(test_tensor).squeeze()\n",
    "    test_probs = torch.sigmoid(test_outputs).cpu().numpy()\n",
    "\n",
    "\n",
    "test_data.loc[valid_idx, 'is_turkey'] = test_probs\n",
    "test_data.loc[valid_idx, ['vid_id', 'is_turkey']].to_csv('result.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turkey_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
